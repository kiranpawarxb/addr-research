"""DatasetGPUProcessor - Advanced GPU processing with HuggingFace dataset optimization.

This module implements the DatasetGPUProcessor class with advanced optimizations including:
- HuggingFace dataset.map() processing to eliminate sequential warnings
- PyTorch model compilation with max-autotune mode
- Half-precision (float16) processing for memory efficiency
- Multiple GPU streams for overlapping execution
- Advanced GPU memory optimization and CUDA optimizations

Requirements: 1.1, 1.2, 2.1, 2.2, 2.3, 2.4, 2.5
"""

import logging
import re
import time
import threading
from typing import List, Dict, Any, Optional, Tuple
import torch
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue, Empty

try:
    from datasets import Dataset
    from transformers import (
        AutoTokenizer, 
        AutoModelForTokenClassification, 
        pipeline,
        TrainingArguments
    )
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False

try:
    from .models import ParsedAddress
    from .hybrid_processor import ProcessingConfiguration, PerformanceMetrics
    from .error_handling import ErrorRecoveryManager
except ImportError:
    # Fallback for direct execution
    from models import ParsedAddress
    from hybrid_processor import ProcessingConfiguration, PerformanceMetrics
    from error_handling import ErrorRecoveryManager

logger = logging.getLogger(__name__)


class DatasetGPUProcessor:
    """Advanced GPU processor with HuggingFace dataset optimization and model compilation.
    
    Implements dataset.map() processing, PyTorch optimizations, and multiple GPU streams
    to achieve maximum throughput with sustained GPU utilization.
    
    Requirements: 1.1, 1.2, 2.1, 2.2, 2.3, 2.4, 2.5
    """
    
    def __init__(self, config: ProcessingConfiguration):
        """Initialize DatasetGPUProcessor with advanced optimizations.
        
        Args:
            config: Processing configuration with GPU settings
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Model components
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.compiled_model = None
        
        # GPU streams for overlapping execution
        self.gpu_streams = []
        self.current_stream_idx = 0
        self.stream_lock = threading.Lock()
        
        # Processing state
        self.is_initialized = False
        self.device = None
        self.model_name = "shiprocket-ai/open-indicbert-indian-address-ner"
        
        # Error handling
        self.error_recovery_manager = None
        
        # Performance tracking
        self.processing_stats = {
            "total_processed": 0,
            "total_batches": 0,
            "compilation_time": 0,
            "dataset_processing_time": 0,
            "gpu_utilization_samples": []
        }
        
        self.logger.info(f"Initialized DatasetGPUProcessor with config: "
                        f"GPU batch size={config.gpu_batch_size}, "
                        f"Dataset batch size={config.dataset_batch_size}, "
                        f"GPU memory fraction={config.gpu_memory_fraction}")
    
    def setup_dataset_gpu_pipeline(self) -> bool:
        """Initialize optimized GPU pipeline with model compilation and optimization.
        
        Sets up:
        - PyTorch model compilation with max-autotune mode
        - Half-precision (float16) processing
        - GPU memory optimization (95%+ allocation)
        - cuDNN benchmarking and TensorFloat-32 operations
        - Multiple GPU streams for overlapping execution
        
        Returns:
            True if setup successful, False otherwise
            
        Requirements: 2.1, 2.2, 2.3, 2.4, 2.5
        """
        if self.is_initialized:
            self.logger.warning("Dataset GPU processor already initialized")
            return True
        
        if not DATASETS_AVAILABLE:
            self.logger.error("HuggingFace datasets not available. Install with: pip install datasets")
            return False
        
        try:
            self.logger.info("üîß Setting up advanced dataset GPU pipeline...")
            
            # Check GPU availability
            if not torch.cuda.is_available():
                self.logger.error("CUDA not available for GPU processing")
                return False
            
            self.device = torch.device("cuda:0")
            gpu_name = torch.cuda.get_device_name(0)
            self.logger.info(f"Using GPU: {gpu_name}")
            
            # Configure PyTorch optimizations
            self._configure_pytorch_optimizations()
            
            # Optimize GPU memory allocation
            self._optimize_gpu_memory()
            
            # Load and optimize model
            if not self._load_and_compile_model():
                return False
            
            # Set up multiple GPU streams
            self._setup_gpu_streams()
            
            # Create optimized pipeline
            self._create_optimized_pipeline()
            
            self.is_initialized = True
            self.logger.info("‚úÖ Dataset GPU pipeline setup completed successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to setup dataset GPU pipeline: {e}")
            return False
    
    def _configure_pytorch_optimizations(self) -> None:
        """Configure PyTorch optimizations for maximum performance.
        
        Enables:
        - cuDNN benchmarking for optimal convolution algorithms
        - TensorFloat-32 (TF32) operations for faster computation
        - Memory allocation optimizations
        
        Requirements: 2.4
        """
        self.logger.info("üîß Configuring PyTorch optimizations...")
        
        # Enable cuDNN benchmarking for optimal performance
        if self.config.enable_cudnn_benchmark:
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            self.logger.info("‚úÖ Enabled cuDNN benchmarking")
        
        # Enable TensorFloat-32 operations for faster computation
        if self.config.enable_tensor_float32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            self.logger.info("‚úÖ Enabled TensorFloat-32 operations")
        
        # Set memory allocation strategy
        torch.cuda.empty_cache()
        self.logger.info("‚úÖ Cleared GPU memory cache")
    
    def _optimize_gpu_memory(self) -> None:
        """Optimize GPU memory allocation for maximum utilization.
        
        Allocates 95%+ of available GPU memory for processing as specified
        in the configuration.
        
        Requirements: 2.3
        """
        self.logger.info("üîß Optimizing GPU memory allocation...")
        
        # Get GPU memory info
        total_memory = torch.cuda.get_device_properties(0).total_memory
        allocated_memory = torch.cuda.memory_allocated(0)
        free_memory = total_memory - allocated_memory
        
        # Calculate target allocation (95%+ of total memory)
        target_allocation = total_memory * self.config.gpu_memory_fraction
        
        self.logger.info(f"GPU Memory - Total: {total_memory / 1e9:.2f}GB, "
                        f"Target allocation: {target_allocation / 1e9:.2f}GB "
                        f"({self.config.gpu_memory_fraction*100:.1f}%)")
        
        # Set memory fraction
        torch.cuda.set_per_process_memory_fraction(self.config.gpu_memory_fraction, device=0)
        self.logger.info("‚úÖ Configured GPU memory allocation")
    
    def _load_and_compile_model(self) -> bool:
        """Load model and apply PyTorch 2.0+ compilation with max-autotune.
        
        Loads the model with half-precision (float16) and applies compilation
        for maximum inference speed.
        
        Returns:
            True if successful, False otherwise
            
        Requirements: 2.1, 2.2
        """
        try:
            self.logger.info("üîß Loading and compiling model...")
            compilation_start = time.time()
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                device_map=None
            )
            self.logger.info("‚úÖ Loaded tokenizer")
            
            # Determine dtype based on configuration
            torch_dtype = torch.float16 if self.config.use_half_precision else torch.float32
            
            # Load model with optimizations
            self.model = AutoModelForTokenClassification.from_pretrained(
                self.model_name,
                torch_dtype=torch_dtype,
                device_map=None,
                trust_remote_code=True
            )
            
            # Move model to GPU
            self.model = self.model.to(self.device)
            self.logger.info(f"‚úÖ Loaded model with {torch_dtype} precision")
            
            # Apply PyTorch 2.0+ compilation if enabled
            if self.config.enable_model_compilation and hasattr(torch, 'compile'):
                self.logger.info("üîß Compiling model with max-autotune mode...")
                
                # Compile with max-autotune for maximum performance
                self.compiled_model = torch.compile(
                    self.model,
                    mode="max-autotune",  # Maximum optimization
                    fullgraph=True,       # Compile entire graph
                    dynamic=False         # Static shapes for better optimization
                )
                
                compilation_time = time.time() - compilation_start
                self.processing_stats["compilation_time"] = compilation_time
                
                self.logger.info(f"‚úÖ Model compilation completed in {compilation_time:.2f}s")
            else:
                self.compiled_model = self.model
                self.logger.info("‚ö†Ô∏è Model compilation skipped (PyTorch 2.0+ required)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to load and compile model: {e}")
            
            # Use error recovery manager for model loading errors
            if self.error_recovery_manager:
                recovery_success = self.error_recovery_manager.handle_model_loading_error(
                    e, self.model_name, "DatasetGPUProcessor"
                )
                if recovery_success:
                    # Retry model loading after recovery
                    return self._retry_model_loading()
            
            return False
    
    def _setup_gpu_streams(self) -> None:
        """Set up multiple GPU streams for overlapping execution.
        
        Creates multiple CUDA streams to enable overlapping computation
        and memory transfers for improved throughput.
        
        Requirements: 2.5
        """
        self.logger.info(f"üîß Setting up {self.config.num_gpu_streams} GPU streams...")
        
        self.gpu_streams = []
        for i in range(self.config.num_gpu_streams):
            stream = torch.cuda.Stream()
            self.gpu_streams.append(stream)

        
        self.logger.info(f"‚úÖ Created {len(self.gpu_streams)} GPU streams for overlapping execution")
    
    def _create_optimized_pipeline(self) -> None:
        """Create optimized HuggingFace pipeline for NER processing."""
        try:
            self.logger.info("üîß Creating optimized NER pipeline...")
            
            # Use original model for pipeline (compiled models not supported by pipeline)
            model_to_use = self.model
            
            # Create pipeline with optimizations
            self.pipeline = pipeline(
                "ner",
                model=model_to_use,
                tokenizer=self.tokenizer,
                device=0,  # Use GPU device 0
                aggregation_strategy="simple"
            )
            
            self.logger.info("‚úÖ Created optimized NER pipeline")
            
        except Exception as e:
            self.logger.error(f"Failed to create pipeline: {e}")
            raise
    
    def process_with_dataset_batching(self, addresses: List[str]) -> List[ParsedAddress]:
        """Process addresses using HuggingFace dataset.map() function.
        
        Uses true dataset batching to eliminate sequential processing warnings
        and achieve optimal GPU utilization with sustained throughput.
        
        Args:
            addresses: List of raw address strings to process
            
        Returns:
            List of ParsedAddress objects with extracted fields
            
        Requirements: 1.1, 1.2, 1.3, 1.4, 1.5
        """
        if not self.is_initialized:
            raise RuntimeError("Dataset GPU processor not initialized. Call setup_dataset_gpu_pipeline() first.")
        
        if not addresses:
            return []
        
        start_time = time.time()
        self.logger.info(f"üöÄ Starting dataset batching processing of {len(addresses)} addresses...")
        
        try:
            # Clean addresses first
            cleaned_addresses = [self._clean_address_text(addr) for addr in addresses]
            
            # Create HuggingFace dataset for optimal GPU processing
            dataset = Dataset.from_dict({"text": cleaned_addresses})
            self.logger.info(f"‚úÖ Created dataset with {len(dataset):,} entries")
            
            # Define processing function for dataset.map()
            def process_batch(batch):
                """Process batch using dataset.map() - eliminates sequential warnings.
                
                This function is called by dataset.map() and processes batches
                of addresses using the GPU pipeline with proper batching.
                """
                batch_texts = batch["text"]
                batch_size = len(batch_texts)
                
                # Process batch on GPU with current stream
                with self._get_next_gpu_stream():
                    try:
                        # Use pipeline for batch processing
                        entities_batch = self.pipeline(batch_texts)
                        
                        # Extract fields for each address in batch
                        parsed_results = []
                        for i, (text, entities) in enumerate(zip(batch_texts, entities_batch)):
                            parsed_address = self._extract_fields_from_ner(text, entities)
                            parsed_address.parse_success = True
                            parsed_results.append(parsed_address)
                        
                        # Convert ParsedAddress objects to dictionaries for serialization
                        parsed_dicts = []
                        for parsed in parsed_results:
                            parsed_dict = {
                                "unit_number": parsed.unit_number,
                                "society_name": parsed.society_name,
                                "landmark": parsed.landmark,
                                "road": parsed.road,
                                "sub_locality": parsed.sub_locality,
                                "locality": parsed.locality,
                                "city": parsed.city,
                                "district": parsed.district,
                                "state": parsed.state,
                                "country": parsed.country,
                                "pin_code": parsed.pin_code,
                                "note": parsed.note,
                                "parse_success": parsed.parse_success,
                                "parse_error": parsed.parse_error
                            }
                            parsed_dicts.append(parsed_dict)
                        
                        # Convert to format expected by dataset.map()
                        return {
                            "parsed_addresses": parsed_dicts,
                            "processing_success": [True] * batch_size,
                            "processing_time": [time.time()] * batch_size
                        }
                        
                    except Exception as e:
                        self.logger.warning(f"Batch processing error: {e}")
                        # Return failed results for this batch
                        failed_results = []
                        for text in batch_texts:
                            failed_address = ParsedAddress(
                                parse_success=False,
                                parse_error=f"Dataset batch processing error: {str(e)}"
                            )
                            failed_results.append(failed_address)
                        
                        # Convert failed ParsedAddress objects to dictionaries
                        failed_dicts = []
                        for failed in failed_results:
                            failed_dict = {
                                "unit_number": failed.unit_number,
                                "society_name": failed.society_name,
                                "landmark": failed.landmark,
                                "road": failed.road,
                                "sub_locality": failed.sub_locality,
                                "locality": failed.locality,
                                "city": failed.city,
                                "district": failed.district,
                                "state": failed.state,
                                "country": failed.country,
                                "pin_code": failed.pin_code,
                                "note": failed.note,
                                "parse_success": failed.parse_success,
                                "parse_error": failed.parse_error
                            }
                            failed_dicts.append(failed_dict)
                        
                        return {
                            "parsed_addresses": failed_dicts,
                            "processing_success": [False] * batch_size,
                            "processing_time": [time.time()] * batch_size
                        }
            
            # Process using TRUE dataset.map() - eliminates sequential warnings
            self.logger.info("üîß Processing with dataset.map() function...")
            dataset_start = time.time()
            
            processed_dataset = dataset.map(
                process_batch,
                batched=True,
                batch_size=self.config.dataset_batch_size,  # Use configured batch size (1000)
                remove_columns=["text"],  # Remove original text column
                desc="Processing addresses with dataset batching"
            )
            
            dataset_processing_time = time.time() - dataset_start
            self.processing_stats["dataset_processing_time"] = dataset_processing_time
            
            # Extract results from processed dataset and convert back to ParsedAddress objects
            results = []
            
            # The processed dataset contains parsed addresses as individual dictionaries
            parsed_addresses_lists = processed_dataset["parsed_addresses"]
            
            for parsed_list in parsed_addresses_lists:
                # Each item is a list of dictionaries (one per address in the batch)
                if isinstance(parsed_list, list):
                    for parsed_dict in parsed_list:
                        if isinstance(parsed_dict, dict):
                            parsed_address = ParsedAddress(
                                unit_number=parsed_dict.get("unit_number", ""),
                                society_name=parsed_dict.get("society_name", ""),
                                landmark=parsed_dict.get("landmark", ""),
                                road=parsed_dict.get("road", ""),
                                sub_locality=parsed_dict.get("sub_locality", ""),
                                locality=parsed_dict.get("locality", ""),
                                city=parsed_dict.get("city", ""),
                                district=parsed_dict.get("district", ""),
                                state=parsed_dict.get("state", ""),
                                country=parsed_dict.get("country", "India"),
                                pin_code=parsed_dict.get("pin_code", ""),
                                note=parsed_dict.get("note", ""),
                                parse_success=parsed_dict.get("parse_success", False),
                                parse_error=parsed_dict.get("parse_error", None)
                            )
                            results.append(parsed_address)
                elif isinstance(parsed_list, dict):
                    # Single dictionary case (when batch_size=1 or single item)
                    parsed_dict = parsed_list
                    parsed_address = ParsedAddress(
                        unit_number=parsed_dict.get("unit_number", ""),
                        society_name=parsed_dict.get("society_name", ""),
                        landmark=parsed_dict.get("landmark", ""),
                        road=parsed_dict.get("road", ""),
                        sub_locality=parsed_dict.get("sub_locality", ""),
                        locality=parsed_dict.get("locality", ""),
                        city=parsed_dict.get("city", ""),
                        district=parsed_dict.get("district", ""),
                        state=parsed_dict.get("state", ""),
                        country=parsed_dict.get("country", "India"),
                        pin_code=parsed_dict.get("pin_code", ""),
                        note=parsed_dict.get("note", ""),
                        parse_success=parsed_dict.get("parse_success", False),
                        parse_error=parsed_dict.get("parse_error", None)
                    )
                    results.append(parsed_address)
            
            # Update statistics
            self.processing_stats["total_processed"] += len(results)
            self.processing_stats["total_batches"] += len(processed_dataset)
            
            total_time = time.time() - start_time
            success_count = sum(1 for r in results if r.parse_success)
            throughput = len(addresses) / total_time
            
            self.logger.info(f"‚úÖ Dataset batching processing completed in {total_time:.2f}s:")
            self.logger.info(f"  Dataset processing time: {dataset_processing_time:.2f}s")
            self.logger.info(f"  Success: {success_count}/{len(addresses)} ({success_count/len(addresses)*100:.1f}%)")
            self.logger.info(f"  Throughput: {throughput:.1f} addresses/second")
            
            # Check if we met performance targets
            if throughput >= 1500:
                self.logger.info(f"üéØ Performance target achieved: {throughput:.1f} >= 1500 addr/sec")
            else:
                self.logger.warning(f"‚ö†Ô∏è Performance below target: {throughput:.1f} < 1500 addr/sec")
            
            return results
            
        except Exception as e:
            self.logger.error(f"Dataset batching processing failed: {e}")
            
            # Check if this is a memory allocation error
            if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
                if self.error_recovery_manager:
                    self.logger.warning("Detected memory allocation error, attempting recovery...")
                    results, new_batch_size = self.error_recovery_manager.handle_memory_allocation_error(
                        addresses, e, self.config.gpu_batch_size, "DatasetGPUProcessor"
                    )
                    
                    # Update batch size for future processing
                    if new_batch_size != self.config.gpu_batch_size:
                        self.config.gpu_batch_size = new_batch_size
                        self.logger.info(f"Updated GPU batch size to {new_batch_size} after memory recovery")
                    
                    return results
            
            # Return failed results for non-memory errors
            return [
                ParsedAddress(
                    parse_success=False,
                    parse_error=f"Dataset processing error: {str(e)}"
                ) for _ in addresses
            ]
    
    def _get_next_gpu_stream(self):
        """Get next GPU stream for overlapping execution.
        
        Returns a context manager that sets the current CUDA stream
        for overlapping computation.
        """
        class StreamContext:
            def __init__(self, stream):
                self.stream = stream
            
            def __enter__(self):
                torch.cuda.set_stream(self.stream)
                return self.stream
            
            def __exit__(self, exc_type, exc_val, exc_tb):
                torch.cuda.synchronize()
        
        with self.stream_lock:
            stream = self.gpu_streams[self.current_stream_idx]
            self.current_stream_idx = (self.current_stream_idx + 1) % len(self.gpu_streams)
        
        return StreamContext(stream)
    
    def _clean_address_text(self, address: str) -> str:
        """Clean and prepare address text for processing.
        
        Args:
            address: Raw address string
            
        Returns:
            Cleaned address text
        """
        if not address:
            return ""
        
        # Remove extra whitespace and limit length
        cleaned = re.sub(r'\s+', ' ', address.strip())
        if len(cleaned) > 300:
            cleaned = cleaned[:300]
        
        return cleaned
    
    def _extract_fields_from_ner(self, raw_address: str, entities) -> ParsedAddress:
        """Extract address fields from NER entities.
        
        Args:
            raw_address: Original address text
            entities: NER entities from the model
            
        Returns:
            ParsedAddress with extracted fields
        """
        # Handle both single entity and list of entities
        if not isinstance(entities, list):
            entities = [entities] if entities else []
        
        # Filter entities by confidence score
        filtered_entities = []
        for entity in entities:
            if isinstance(entity, dict) and entity.get('score', 0) > 0.5:
                filtered_entities.append(entity)
        
        # Initialize fields
        fields = {
            'unit_number': '',
            'society_name': '',
            'landmark': '',
            'road': '',
            'sub_locality': '',
            'locality': '',
            'city': '',
            'district': '',
            'state': '',
            'country': 'India',
            'pin_code': ''
        }
        
        # Extract from NER entities
        for entity in filtered_entities:
            entity_type = entity.get('entity_group', '').lower()
            entity_text = entity.get('word', '').strip().rstrip(',').strip()
            
            # Map entity types to fields (first match wins)
            if entity_type in ['house_details', 'house_number', 'flat', 'unit'] and not fields['unit_number']:
                fields['unit_number'] = entity_text
            elif entity_type in ['building_name', 'building', 'society', 'complex'] and not fields['society_name']:
                fields['society_name'] = entity_text
            elif entity_type in ['landmarks', 'landmark', 'near'] and not fields['landmark']:
                fields['landmark'] = entity_text
            elif entity_type in ['street', 'road'] and not fields['road']:
                fields['road'] = entity_text
            elif entity_type in ['sublocality', 'sub_locality', 'area'] and not fields['sub_locality']:
                fields['sub_locality'] = entity_text
            elif entity_type in ['locality', 'neighbourhood', 'neighborhood'] and not fields['locality']:
                fields['locality'] = entity_text
            elif entity_type in ['city', 'town'] and not fields['city']:
                fields['city'] = entity_text
            elif entity_type in ['district'] and not fields['district']:
                fields['district'] = entity_text
            elif entity_type in ['state', 'province'] and not fields['state']:
                fields['state'] = entity_text
            elif entity_type in ['pincode', 'postcode', 'zip', 'pin_code'] and not fields['pin_code']:
                fields['pin_code'] = entity_text
        
        # Fallback PIN code extraction using regex
        if not fields['pin_code']:
            pin_match = re.search(r'\b(\d{6})\b', raw_address)
            if pin_match:
                fields['pin_code'] = pin_match.group(1)
        
        # Set district to city if not found
        if not fields['district'] and fields['city']:
            fields['district'] = fields['city']
        
        return ParsedAddress(
            unit_number=fields['unit_number'],
            society_name=fields['society_name'],
            landmark=fields['landmark'],
            road=fields['road'],
            sub_locality=fields['sub_locality'],
            locality=fields['locality'],
            city=fields['city'],
            district=fields['district'],
            state=fields['state'],
            country=fields['country'],
            pin_code=fields['pin_code'],
            note="Parsed using DatasetGPUProcessor with HuggingFace dataset.map()",
            parse_success=False,  # Will be set by caller
            parse_error=None
        )
    
    def get_gpu_utilization(self) -> float:
        """Get current GPU utilization percentage.
        
        Returns:
            GPU utilization as percentage (0-100)
        """
        try:
            if torch.cuda.is_available():
                # Get GPU utilization using nvidia-ml-py if available
                try:
                    import pynvml
                    pynvml.nvmlInit()
                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                    utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    return float(utilization.gpu)
                except ImportError:
                    # Fallback: estimate based on memory usage
                    allocated = torch.cuda.memory_allocated(0)
                    total = torch.cuda.get_device_properties(0).total_memory
                    return (allocated / total) * 100
            return 0.0
        except Exception:
            return 0.0
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """Get comprehensive processing statistics.
        
        Returns:
            Dictionary with processing statistics and performance metrics
        """
        current_utilization = self.get_gpu_utilization()
        
        return {
            "is_initialized": self.is_initialized,
            "device": str(self.device) if self.device else None,
            "model_compiled": self.compiled_model is not None,
            "gpu_streams": len(self.gpu_streams),
            "total_processed": self.processing_stats["total_processed"],
            "total_batches": self.processing_stats["total_batches"],
            "compilation_time": self.processing_stats["compilation_time"],
            "dataset_processing_time": self.processing_stats["dataset_processing_time"],
            "current_gpu_utilization": current_utilization,
            "configuration": {
                "gpu_batch_size": self.config.gpu_batch_size,
                "dataset_batch_size": self.config.dataset_batch_size,
                "gpu_memory_fraction": self.config.gpu_memory_fraction,
                "num_gpu_streams": self.config.num_gpu_streams,
                "use_half_precision": self.config.use_half_precision,
                "enable_model_compilation": self.config.enable_model_compilation,
                "enable_cudnn_benchmark": self.config.enable_cudnn_benchmark,
                "enable_tensor_float32": self.config.enable_tensor_float32
            }
        }
    
    def shutdown(self) -> None:
        """Gracefully shutdown the GPU processor and cleanup resources."""
        self.logger.info("Shutting down DatasetGPUProcessor...")
        
        try:
            # Clear GPU memory
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # Reset state
            self.model = None
            self.tokenizer = None
            self.pipeline = None
            self.compiled_model = None
            self.gpu_streams = []
            self.is_initialized = False
            
            self.logger.info("‚úÖ DatasetGPUProcessor shutdown completed")
            
        except Exception as e:
            self.logger.error(f"Error during shutdown: {e}")
    
    def _retry_model_loading(self) -> bool:
        """Retry model loading after error recovery."""
        try:
            # Simplified retry with basic configuration
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForTokenClassification.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,  # Use float32 for stability
                trust_remote_code=False
            )
            self.model = self.model.to(self.device)
            self.compiled_model = self.model  # Skip compilation on retry
            
            self.logger.info("‚úÖ Model loading retry successful")
            return True
            
        except Exception as e:
            self.logger.error(f"Model loading retry failed: {e}")
            return False
    
    def set_error_recovery_manager(self, error_manager: 'ErrorRecoveryManager') -> None:
        """Set error recovery manager for comprehensive error handling."""
        self.error_recovery_manager = error_manager