# ============================================================================
# Address Consolidation System - Configuration File
# ============================================================================
# This configuration file controls all aspects of the address consolidation
# system including input/output paths, LLM parsing settings, consolidation
# rules, and logging behavior.
#
# Environment variables can be substituted using ${VAR_NAME} syntax.
# ============================================================================

# ----------------------------------------------------------------------------
# INPUT CONFIGURATION
# ----------------------------------------------------------------------------
# Controls how the system reads and validates input CSV files.
input:
  # Path to the input CSV file containing address data
  # Can be absolute or relative to the working directory
  # Example: "data/addresses.csv" or "/path/to/export_customer_address_store_p0.csv"
  file_path: "export_customer_address_store_p0.csv"
  
  # List of required columns that must be present in the CSV file
  # The system will validate these columns exist before processing
  # Missing columns will cause an error and prevent processing
  # Requirements: 1.2, 1.3
  required_columns:
    - addr_text      # Raw address text to be parsed
    - pincode        # PIN code for geographic matching
    - city_id        # City identifier from source system
    # Optional additional columns that may be useful:
    # - state_id
    # - zone_id
    # - address_id
    # - addr_hash_key

# ----------------------------------------------------------------------------
# LLM PARSER CONFIGURATION
# ----------------------------------------------------------------------------
# Controls how the system parses addresses.
# Requirements: 6.1, 7.1
llm:
  # Parser type: "openai", "local", "indicbert", "shiprocket", or "libpostal"
  # - "openai": Use OpenAI API (requires API key and internet connection)
  # - "local": Use local rule-based parser (runs offline, no API costs, fastest)
  # - "indicbert": Use IndicBERT transformer model (offline, ML-based, ~500MB download)
  # - "shiprocket": Use Shiprocket's fine-tuned IndicBERT for Indian addresses (best for Indian addresses)
  # - "libpostal": Use libpostal statistical NLP parser (requires C library)
  # Recommended: "local" for speed, "shiprocket" for best Indian address accuracy
  # LOCAL: Fast, rule-based, optimized for Indian addresses
  # SHIPROCKET: ML-based, specifically trained for Indian addresses
  # INDICBERT: General ML-based, good for complex addresses
  parser_type: "local"
  
  # LLM API endpoint URL (only used when parser_type is "openai")
  # For OpenAI: "https://api.openai.com/v1/chat/completions"
  # For Azure OpenAI: "https://<resource>.openai.azure.com/openai/deployments/<deployment>/chat/completions?api-version=2023-05-15"
  # For local models: "http://localhost:8000/v1/chat/completions"
  api_endpoint: "https://api.openai.com/v1/chat/completions"
  
  # API key for authentication (only used when parser_type is "openai")
  # Use environment variable substitution for security: ${OPENAI_API_KEY}
  # Never commit actual API keys to version control
  # Set the environment variable before running: export OPENAI_API_KEY="sk-..."
  api_key: "${OPENAI_API_KEY}"
  
  # Model name to use for parsing (only used when parser_type is "openai")
  # OpenAI options: "gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"
  # Choose based on accuracy needs vs. cost:
  # - gpt-4: Highest accuracy, best for complex addresses
  # - gpt-3.5-turbo: Faster and cheaper, good for simpler addresses
  model: "gpt-4"
  
  # Local model name (only used when parser_type is "local")
  # Hugging Face model for Indian text understanding
  # Default: "ai4bharat/indic-bert" (optimized for Indian languages)
  local_model: "ai4bharat/indic-bert"
  
  # Maximum number of retry attempts for failed API calls
  # The system will retry on timeouts and transient errors
  # Retries use exponential backoff to avoid overwhelming the API
  # Recommended: 3 (provides good balance between reliability and speed)
  # Requirement: 7.1
  max_retries: 3
  
  # Timeout in seconds for each API request
  # If the API doesn't respond within this time, the request is retried
  # Recommended: 30 seconds for most use cases
  # Increase for slower networks or complex parsing tasks
  timeout_seconds: 30
  
  # Number of addresses to process in parallel
  # Higher values improve throughput but may hit rate limits
  # Recommended: 10 for OpenAI, adjust based on your rate limits
  # Set to 1 to disable parallel processing
  # Requirement: 6.4
  batch_size: 10

# ----------------------------------------------------------------------------
# CONSOLIDATION ENGINE CONFIGURATION
# ----------------------------------------------------------------------------
# Controls how addresses are grouped based on Society Name and PIN code.
# Requirements: 6.2, 6.3
consolidation:
  # Enable fuzzy matching for Society Names
  # When true, uses string similarity algorithms to match similar names
  # Example: "Prestige Lakeside" matches "Prestige Lake Side"
  # When false, requires exact matches (after normalization)
  # Recommended: true for real-world data with spelling variations
  # Requirement: 6.2
  fuzzy_matching: true
  
  # Similarity threshold for fuzzy matching (0.0 to 1.0)
  # Only used when fuzzy_matching is enabled
  # Two Society Names are grouped if similarity >= threshold
  # Higher values = stricter matching (fewer false positives)
  # Lower values = looser matching (more groups consolidated)
  # Recommended: 0.85 (good balance for Indian addresses)
  # Range: 0.70 (loose) to 0.95 (strict)
  # Requirement: 6.3
  similarity_threshold: 0.85
  
  # Enable normalization of Society Names before matching
  # Normalization: lowercase, trim whitespace, remove special characters
  # Example: "Prestige Lake-Side!" becomes "prestige lakeside"
  # Recommended: true (improves matching accuracy)
  normalize_society_names: true

# ----------------------------------------------------------------------------
# OUTPUT CONFIGURATION
# ----------------------------------------------------------------------------
# Controls how consolidated results are written to CSV files.
output:
  # Path to the output CSV file
  # Can be absolute or relative to the working directory
  # The file will be created if it doesn't exist
  # Existing files will be overwritten
  # Example: "output/consolidated_addresses.csv"
  file_path: "consolidated_addresses.csv"
  
  # Include statistics summary in console output
  # When true, displays consolidation statistics after processing
  # Statistics include: total groups, average per group, largest group, etc.
  # Recommended: true for interactive use, false for automated pipelines
  include_statistics: true

# ----------------------------------------------------------------------------
# LOGGING CONFIGURATION
# ----------------------------------------------------------------------------
# Controls logging behavior for debugging and monitoring.
logging:
  # Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  # DEBUG: Detailed information for debugging (verbose)
  # INFO: General informational messages (recommended for normal use)
  # WARNING: Warning messages for potential issues
  # ERROR: Error messages for failures
  # CRITICAL: Critical errors that may cause system failure
  # Recommended: INFO for production, DEBUG for troubleshooting
  level: "INFO"
  
  # Path to the log file
  # Logs are written to both console and file
  # Can be absolute or relative to the working directory
  # The file will be created if it doesn't exist
  # Logs are appended to existing files
  # Example: "logs/consolidation.log"
  file_path: "consolidation.log"

# ============================================================================
# USAGE EXAMPLES
# ============================================================================
#
# Basic usage with defaults:
#   python -m src --config config/config.yaml --input data.csv --output results.csv
#
# With custom configuration:
#   python -m src --config config/custom.yaml --input data.csv --output results.csv
#
# With environment variables:
#   export OPENAI_API_KEY="sk-..."
#   python -m src --config config/config.yaml --input data.csv --output results.csv
#
# With verbose logging:
#   python -m src --config config/config.yaml --input data.csv --output results.csv --verbose
#
# ============================================================================
# TROUBLESHOOTING
# ============================================================================
#
# Issue: "Missing required columns" error
# Solution: Check that your CSV has addr_text, pincode, and city_id columns
#
# Issue: API authentication errors
# Solution: Verify OPENAI_API_KEY environment variable is set correctly
#
# Issue: Timeout errors
# Solution: Increase timeout_seconds or reduce batch_size
#
# Issue: Rate limit errors
# Solution: Reduce batch_size to make fewer parallel requests
#
# Issue: Too many/too few consolidated groups
# Solution: Adjust similarity_threshold (higher = fewer groups, lower = more groups)
#
# ============================================================================
